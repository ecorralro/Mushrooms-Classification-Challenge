# Lib import
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import charset_normalizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve

import warnings
warnings.filterwarnings('ignore')


with open("train.csv", 'rb') as rawdata:
    result = charset_normalizer.detect(rawdata.read(10000))
print(result)


with open("test.csv", 'rb') as rawdata:
    result = charset_normalizer.detect(rawdata.read(10000))
print(result)


df = pd.read_csv("train.csv", encoding = "utf-8")
df.head()



df_test = pd.read_csv("test.csv", encoding = "utf-8")
df_test.head()



df.info()





# Revisar valores nulos
print("Valores nulos en Train Dataset:\n", df.isnull().sum())

# Identificar duplicados
print("\nFilas duplicadas en Train Dataset:", df.duplicated().sum())

# Tipos de datos
print("\nTipos de datos del Train Dataset:\n", df.dtypes)

# Explorar valores únicos de las columnas categóricas
categorical_columns = df.select_dtypes(include=['object']).columns
for col in categorical_columns:
    print(f"\nValores únicos en {col}:\n", df[col].unique())


# Revisar valores nulos
print("Valores nulos en Train Dataset:\n", df_test.isnull().sum())

# Identificar duplicados
print("\nFilas duplicadas en Train Dataset:", df_test.duplicated().sum())

# Tipos de datos
print("\nTipos de datos del Train Dataset:\n", df_test.dtypes)

# Explorar valores únicos de las columnas categóricas
categorical_columns = df_test.select_dtypes(include=['object']).columns
for col in categorical_columns:
    print(f"\nValores únicos en {col}:\n", df_test[col].unique())


stalk_root_count = (df['stalk-root']== '?').sum()
print(stalk_root_count)





sns.histplot(data = df['class'])
plt.title('Histogram of Class for looking imbalace')
plt.show








from sklearn.preprocessing import LabelEncoder, OrdinalEncoder

label_encoders = {}

label_columns = ['cap-shape', 'cap-surface', 'cap-color', 'bruises',
       'odor', 'gill-attachment', 'gill-spacing', 'gill-color',
       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',
       'stalk-surface-below-ring', 'stalk-color-above-ring',
       'stalk-color-below-ring', 'veil-type', 'veil-color',
       'ring-type', 'spore-print-color', 'habitat','gill-size', 'ring-number',
       'population', 'class']


for col in label_columns:
    label_encoder = LabelEncoder()
    df[col] = label_encoder.fit_transform(df[col])
    if col != "class":
        df_test[col] = label_encoder.transform(df_test[col])
    label_encoders[col] = label_encoder



df.hist(bins=10, figsize=(14, 12))
plt.show()





df_corr = df.corr()

plt.figure(figsize=(30, 20))
sns.heatmap(df_corr,annot=True, cmap='coolwarm')
plt.show()





from sklearn.feature_selection import SelectKBest, f_classif

df_fs = df.select_dtypes(include=['int', 'float'])
selector = SelectKBest(score_func=f_classif, k=10)
selector.fit_transform(df_fs, df['class'])
selected_columns = df_fs.columns[selector.get_support()]
selected_data = df_fs[selected_columns]

print("Selected Features Shape:", selected_columns.shape)





df_corr_fs = df_fs[selected_columns].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(df_corr_fs,annot=True, cmap='coolwarm')
plt.show()








from sklearn.model_selection import train_test_split

X_fs = df_fs[selected_columns].drop('class', axis=1)
y_fs = df_fs['class']


X_train_fs, X_test_fs, y_train_fs, y_test_fs = train_test_split(X_fs, y_fs, test_size=0.2, random_state=42)



X = df.drop('class', axis = 1)
y = df['class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)





from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import log_loss,classification_report
from sklearn.model_selection import cross_val_score

models = {
    "Logistic Regression": LogisticRegression(),
    'SVM': SVC(),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'KNN': KNeighborsClassifier()
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"Modelo: {name}")
    print(classification_report(y_test, y_pred))
    print(confusion_matrix(y_test, y_pred))
    print("-" * 50)


for name, model in models.items():
    cv_score = cross_val_score(model, X, y, cv=5)
    print(f"{name} - CV Score: {cv_score.mean()}")


### Peores resultados con feature selection
models = {
    "Logistic Regression": LogisticRegression(),
    'SVM': SVC(),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'KNN': KNeighborsClassifier()
}

for name, model in models.items():
    model.fit(X_train_fs, y_train_fs)
    y_pred = model.predict(X_test_fs)
    print(f"Modelo: {name}")
    print(classification_report(y_test_fs, y_pred))
    print(confusion_matrix(y_test_fs, y_pred))
    print("-" * 50)


for name, model in models.items():
    cv_score = cross_val_score(model, X_fs, y_fs, cv=5)
    print(f"{name} - CV Score: {cv_score.mean()}")





from sklearn.linear_model import LogisticRegression


model_lr = LogisticRegression()

# Train the model
model_lr.fit(X_train_fs, y_train_fs)

# Make predictions
y_pred = model_lr.predict(X_test_fs)         # Get predicted labels
y_pred_proba = model_lr.predict_proba(X_test_fs)  # Get predicted probabilities

# Calculate log-loss
logloss = log_loss(y_test_fs, y_pred_proba)

# Print results
print(f'Predicted Labels: {y_pred}')
print(f'Predicted Probabilities: {y_pred_proba}')
print(f'Log-Loss: {logloss:.2f}')






# Calculate metrics
accuracy = accuracy_score(y_test_fs, y_pred)
precision = precision_score(y_test_fs, y_pred)
auc = roc_auc_score(y_test_fs, y_pred_proba[:, 1])
recall = recall_score(y_test_fs, y_pred)
f1 = f1_score(y_test_fs, y_pred)

# Print metrics
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"AUC: {auc}")


# Compute the confusion matrix
cm = confusion_matrix(y_test_fs, y_pred)

# Visualize the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_lr.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()


model_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)
model_l1.fit(X_train, y_train)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred_proba[:, 1])
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print metrics
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"AUC: {auc}")


model_l2 = LogisticRegression(penalty='l2', C=1.0)
model_l2.fit(X_train, y_train)

# Calculate metrics
accuracy_l2 = accuracy_score(y_test, y_pred)
precision_l2 = precision_score(y_test, y_pred)
auc_l2 = roc_auc_score(y_test, y_pred_proba[:, 1])
recall_l2 = recall_score(y_test, y_pred)
f1_l2 = f1_score(y_test, y_pred)

# Print metrics
print(f"Accuracy: {accuracy_l2}")
print(f"Precision: {precision_l2}")
print(f"Recall: {recall_l2}")
print(f"F1 Score: {f1_l2}")
print(f"AUC: {auc_l2}")





# Entrenamiento del modelo
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Evaluación
y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy (Random Forest): {accuracy}")





df.head()


df_test.head()


X_test



y_test_pred = rf_model.predict(df_test)

y_test_pred = ["p" if i == 1 else "e" for i in y_test_pred]
# Crear un DataFrame con los IDs y las predicciones
submission = pd.DataFrame({
    'ID': range(1, len(y_test_pred) + 1),  # Crear una columna de IDs secuenciales
    'Edible': y_test_pred
})

# Guardar el archivo como 'submission_labels.csv'
submission.to_csv('submission_labels.csv', index=False)

print("Archivo 'submission_labels.csv' generado exitosamente.")
